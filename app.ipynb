{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import networkx as nx\n",
    "from spektral.utils import normalized_adjacency\n",
    "import requests\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Conv1D, MaxPooling1D, Dropout, Input, Flatten\n",
    "from spektral.layers import GCSConv, GlobalAvgPool, MinCutPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes (e.g., Phishing or Not Phishing)\n",
    "num_classes = 2\n",
    "num_nodes, num_features = 140, 140\n",
    "\n",
    "#gcsconv activation func\n",
    "gcsconv_act = 'relu'\n",
    "#output activation func\n",
    "out_act = 'linear'\n",
    "#optimizer\n",
    "optimizer = 'adam'\n",
    "\n",
    "# Inputs\n",
    "X_in = Input(shape=(num_nodes,num_features))\n",
    "A_in = Input(shape=(num_nodes,num_features), sparse=True)\n",
    "#I_in = Input(shape=(None,),name='segment_ids',dtype=tf.int32)\n",
    "\n",
    "class GraphConvSkip(GCSConv):\n",
    "    def __init__(self,channels,activation='relu'):\n",
    "        super().__init__(channels,activation=activation)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config\n",
    "#GCSConv(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-3), use_bias=True, kernel_initializer='he_uniform', bias_initializer='zeros')\n",
    "# Graph Convolution and Pooling layers\n",
    "gc1 = GraphConvSkip(channels=32)([X_in, A_in])\n",
    "dropout_1 = Dropout(.5)(gc1)\n",
    "gc2 = GraphConvSkip(channels=32)([dropout_1, A_in])\n",
    "dropout_2 = Dropout(.5)(gc2)\n",
    "gc3 = GraphConvSkip(channels=32)([dropout_2, A_in])\n",
    "\n",
    "# GlobalAvgPool layer\n",
    "global_avg_pool = GlobalAvgPool()(gc3)\n",
    "\n",
    "# Classification layer\n",
    "output = Dense(num_classes, activation='linear')(global_avg_pool)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=[X_in, A_in], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_graphs(graph):\n",
    "    # Compute the maximum number of nodes among all graphs\n",
    "    max_nodes = 140\n",
    "    max_features = 140\n",
    "\n",
    "    adj_matrix = nx.to_numpy_array(graph, dtype=np.float32)\n",
    "    adj_matrix = np.pad(adj_matrix, ((0, max_nodes - adj_matrix.shape[0]), (0, max_nodes - adj_matrix.shape[1])), mode='constant')\n",
    "    adj_matrix = normalized_adjacency(adj_matrix)\n",
    "\n",
    "\n",
    "    nodes = np.eye(len(graph.nodes()), dtype=np.float32)\n",
    "    nodes = np.pad(nodes, ((0, max_nodes - nodes.shape[0]), (0, max_features - nodes.shape[1])), mode='constant')\n",
    "\n",
    "    return np.array(adj_matrix), np.array(nodes)\n",
    "\n",
    "def graph_maker(url):\n",
    "    # Read HTML file and convert it to a NetworkX graph\n",
    "    try:\n",
    "        html = Request(url,headers={'User-Agent':'Mozilla/5.0'})\n",
    "        html_object = urlopen(html).read()\n",
    "        soup = bs(html_object,'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except HTTPError as e:\n",
    "        print(f'Error{e}')\n",
    "\n",
    "    #transform html into graph form\n",
    "    G = nx.Graph()\n",
    "    for tag in soup.find_all():\n",
    "        G.add_node(tag.name)\n",
    "    for tag in soup.find_all():\n",
    "        for child in tag.find_all():\n",
    "            G.add_edge(tag.name, child.name)\n",
    "\n",
    "    adj_matrix, node_features = preprocess_graphs(G)\n",
    "\n",
    "    return adj_matrix, node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def URLDET(url):\n",
    "    f = open(r'D:\\Kerja\\UITM\\Sem_6\\CSP_650\\FYP\\urldet_vocab.json')\n",
    "    vocab = json.load(f)\n",
    "    urldet = keras.models.load_model(r'D:\\Kerja\\UITM\\Sem_6\\CSP_650\\FYP\\urldet.h5')\n",
    "\n",
    "    #preprocess html\n",
    "    adj_matrix, node_features = graph_maker(url)\n",
    "    adj_matrix = np.array([adj_matrix])\n",
    "    node_features = np.array([node_features])\n",
    "    #predict html\n",
    "    try:\n",
    "        html_prediction = model.predict([node_features,adj_matrix])\n",
    "        max_html = np.where(html_prediction[0] == np.max(html_prediction[0]))\n",
    "    except HTTPError as e:\n",
    "        max_html = np.array([1])\n",
    "        html_prediction = np.array([[0,0]])\n",
    "    except ValueError as e:\n",
    "        max_html = np.array([1])\n",
    "        html_prediction = np.array([[0,0]])\n",
    "    \n",
    "    #url cutter\n",
    "    if len(url) < 150:\n",
    "        url = url + (' '*(150-len(url)))\n",
    "    url = url[:150]\n",
    "\n",
    "    #convert url into vector\n",
    "    vector_url = [vocab[char] for char in url]\n",
    "\n",
    "    #predict the url\n",
    "    url_prediction = urldet.predict(np.array([vector_url]))\n",
    "    max_url = np.where(url_prediction[0] == np.max(url_prediction[0]))\n",
    "\n",
    "    prediction = [max_url,max_html]\n",
    "    if np.sum(prediction) == 2:\n",
    "        predict = 'Safe'\n",
    "    elif np.sum(prediction) == 1:\n",
    "        predict = \"Careful\"\n",
    "    else:\n",
    "        predict = 'Phishing'\n",
    "        \n",
    "    return predict, url_prediction, html_prediction\n",
    "\n",
    "demo = gr.Interface(URLDET, inputs=[gr.Textbox(label='URL',lines=2,placeholder='url')], \n",
    "                    outputs=[gr.Textbox(label='prediction'),gr.Textbox(label='URLDET result'),gr.Textbox(label='HTMLDET prediction')],\n",
    "                    examples=['https://www.cbsnews.com/news/twitter-rebrand-x-name-change-elon-musk-what-it-means/','https://freefireadvanceserver.info','https://alw4erb.web.app/'],\n",
    "                    theme=gr.themes.Soft())\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
